%% Chapter 10 %%%
\chapter{Скрываем данные}
\label{chp10}

\begin{chapquote}{Эрик С. Рэймонд}
``При достаточном количестве глаз, все ошибки выплывают на поверхность.''
\end{chapquote}


Ранее мы говорили о наиболее распространенных ошибках, которые допускают учёные, и о том, что наилучший способ их обнаружить - тщательное изучение извне. Экспертная оценка, в некоторой степени этому способствует, однако у человека, проводящего оценку, недостаточно времени для того, чтобы подробно проанализировать все данные повторно и проверить все исходные коды анализа на ошибки, - рецензенты могут лишь проверить правильность выбранной методологии исследования. Иногда они замечают очевидные ошибки, но едва заметные проблемы, как правило, пропускают. \cite{schroter_what_2008}  


Поэтому многие рецензируемые журналы и профессиональные сообщества требуют от исследователей предоставлять другим исследователям доступ к своим данным по запросу. Полные наборы данных, как правило, слишком большие по размеру, чтобы их можно было напечатать на страницах журнала, поэтому авторы лишь публикуют свои результаты в статьях, а копию данных отправляют другим исследователям по запросу. Возможно, другие исследователи смогут обнаружить ошибку или незамеченную закономерность.

Теоретически, именно так это и должно происходить. В 2005 году Джелте Уичертс с коллегами из университета Амстердама решили проанализировать недавно опубликованные статьи в нескольких знаменитых журналов Американской Психологической Ассоциации, чтобы узнать об использованных в них статистических методах. Они выбрали журналы АПА отчести потому, что это сообщество требует от авторов статей согласие на предоставление собранных в рамках исследования данных другим психологам, стремящихся проверить их результаты. 

Шесть месяцев спустя они смогли получить данные только по 64 исследованиям из 249 анализируемых. Почти три четверти авторов статей так и не прислали им свои данные. \cite{wicherts_poor_2006} 

Конечно, учёные - занятые люди и, возможно, у них просто не нашлось свободного времени, чтобы собрать свои даные, подготовить пояснительные документы, описывающие значения каждой переменной и каким образом она была измерена, и так далее.

Уитчерс и коллеги решили это проверить. Они тщательно просмотрели все исследования на наличие наиболее распространенных ошибок, которые можно было бы заметить в процессе чтения статьи, таких как: противоречивые статистические результаты, неправильное применение различных статистических тестов и обычных опечаток. По крайней мере, в половине статей содержались ошибки, как правило, незначительные, однако 15\% статей содержали описание не менее одного статистически значимого результата, который был значимым исключительно из-за ошибки. 

Далее, они искали корреляцию между этими ошибками и нежеланием авторов делиться своими данными, и, как оказалось, между ними существовала четкая связь. Авторы, отказавшиеся делиться своими данными, были в большей степени склонны допускать ошибку в своей статье, и статистические обоснования их выводов были, как правило, слабее. \cite{wicherts_willingness_2011} Поскольку большинство авторов отказались предоставлять свои данные, Уитчерс не мог провести более детальный поиск статистических ошибок, которых могло оказаться гораздо больше.

Это, конечно, не доказательство того, что авторы скрывали свои данные, боясь обнаружения их ошибок, или что они вообще знали о наличии этих ошибок. Наличие корреляции не подразумевает наличие причинно-следственной связи, но в данном случае корреляция явно намекает ``внимательно посмотри сюда.''\footnote{Шутка бесстыдно украдена из альтернативного варианта комикса \href{http://xkcd.com/552/}{http://xkcd.com/552/}}



\section{Просто опустите подробности}
\label{chp10:leaveoutdetails}

Придирчивые статистики тянут вас на дно, указывая на недостатки вашей статьи? Существует одно простое решение: публикуйте как можно меньше подробностей! Они не смогут найти ошибки, если вы не скажете как вы оценивали свои данные.

Я не утверждаю всерьёз, что злые ученые делают это намерянно, хотя, возможно, некоторые делают. Чаще детали опущены просто потому, что авторы забыли включить их в статью или в виду того, что ограничения журнала на размер статьи заставили так поступить.

Исследование можно оценить, чтобы понять, что было исключено. Учёные, ведущие медицинские испытания, должны предоставлять подробные планы исследования экспертным советам по этике до начала испытаний, поэтому одна группа исследователей получила коллекцию таких планов от одного  экспертного совета. В этих планах указывалось, какие результаты исследования будут измеряны: например, можно отслеживать различные симптомы, чтобы пронаблюдать, оказывало ли на них влияние тестируемое лекарство. Затем исследователи разыскали опубликованные результаты этих планируемых исследований и посмотрели, насколько хорошо эти результаты были представлены.   

Примерно половина планируемых результатов никогда так и не появились в научных статьях рецензируемых журналов. Многие из них оказались статистически незначимыми, поэтому были просто скрыты. Другая большая часть результатов была опубликована без подробностей, что исключало возможность в дальнейшем использовать эти результаты для мета-анализа. \cite{chan_empirical_2004} 

%убрал сноску №2, не вижу смысла ее переводить %%%

Другие обзоры сталкивались с похожими проблемами. Обзор клинических испытаний обнаружил, что большинство исследований опускают важные методологические детали, как например, \hyperref[chp7]{правила остановки} или \hyperref[chp3]{расчеты мощности}, - это свойственно, в большей степени, небольшим специализированным журналам, нежели большим общемедицинским журналам. \cite{huwiler-muntener_quality_2002}

Журналы по медицине пытаются бороться с этой проблемой путем стандартизации отчетов о результатах, таких как список \href{http://www.consort-statement.org/}{CONSORT}. Авторы должны следовать требованиям списка до момента предоставления работы на рассмотрение, а редакторы проверяют, включены ли в статью все необходимые детали. Этот список, похоже, работает: исследования, опубликованные в журналах, которые следуют рекомендациям от CONSORT, как правило, предоставляют больше существенных подробностей об исследовании, хотя и не все. \cite{plint_does_2006} К сожалению, стандарты не всегда последовательно применяются и некоторым работам удается проскочить мимо них с отсутствующими в исследовании деталями. \cite{mills_analysis_2005} Редакторам журналов придётся приложить больше усилий для соблюдения стандартов отчётности.


We see that published papers aren’t faring very well. What about unpublished studies?

\section{Science in a filing cabinet}
\label{chp10:sciencecabinet}


Earlier we saw the impact of multiple comparisons and truth inflation on study results. These problems arise when studies make numerous comparisons with low statistical power, giving a high rate of false positives and inflated estimates of effect sizes, and they appear everywhere in published research.

But not every study is published. We only ever see a fraction of medical research, for instance, because few scientists bother publishing “We tried this medicine and it didn’t seem to work.”

Consider an example: studies of the tumor suppressor protein TP53 and its effect on head and neck cancer. A number of studies suggested that measurements of TP53 could be used to predict cancer mortality rates, since it serves to regulate cell growth and development and hence must function correctly to prevent cancer. When all 18 published studies on TP53 and cancer were analyzed together, the result was a highly statistically significant correlation: TP53 could clearly be measured to tell how likely a tumor is to kill you.

But then suppose we dig up unpublished results on TP53: data that had been mentioned in other studies but not published or analyzed. Add this data to the mix and the statistically significant effect vanishes.36 After all, few authors bothered to publish data showing no correlation, so the meta-analysis could only use a biased sample.

A similar study looked at reboxetine, an antidepressant sold by Pfizer. Several published studies have suggested that it is effective compared to placebo, leading several European countries to approve it for prescription to depressed patients. The German Institute for Quality and Efficiency in Health Care, responsible for assessing medical treatments, managed to get unpublished trial data from Pfizer – three times more data than had ever been published – and carefully analyzed it. The result: reboxetine is not effective. Pfizer had only convinced the public that it’s effective by neglecting to mention the studies proving it isn’t.18

This problem is commonly known as publication bias or the file-drawer problem: many studies sit in a file drawer for years, never published, despite the valuable data they could contribute.

The problem isn’t simply the bias on published results. Unpublished studies lead to a duplication of effort – if other scientists don’t know you’ve done a study, they may well do it again, wasting money and effort.

Regulators and scientific journals have attempted to halt this problem. The Food and Drug Administration requires certain kinds of clinical trials to be registered through their website ClinicalTrials.gov before the trials begin, and requires the publication of results within a year of the end of the trial. Similarly, the International Committee of Medical Journal Editors announced in 2005 that they would not publish studies which had not been pre-registered.

Unfortunately, a review of 738 registered clinical trials found that only 22\% met the legal requirement to publish.47 The FDA has not fined any drug companies for noncompliance, and journals have not consistently enforced the requirement to register trials. Most studies simply vanish.
[1]	Joke shamelessly stolen from the alternate text of http://xkcd.com/552/.
[2]	Why do we always say “swept under the rug”? Whose rug is it? And why don’t they use a vacuum cleaner instead of a broom?