%% Chapter 9 %%%
\chapter{Ошибки делают все}
\label{chp9}

До сих пор, я предполагал, что ученые способны выполнять статистические вычисления с идеальной точностью и ошибаться могут только в выборе подходящих для этих вычислений цифр. Ученые могут неправильно использовать результаты статистического анализа или не в состоянии выполнить соответствующие расчеты, но они ведь могут, как минимум, правильно рассчитать \emph{p-}значение?

Возможно, нет. 

Обзоры статистически значимых результатов, представленных в медицинских и психологических исследованиях, показывают, что многие \emph{p-}значения ошибочны, и некоторые статистически незначимые результаты на самом деле значимы, если правильно их пересчитать. \cite{gotzsche_believability_2006,bakker_misreporting_2011} Другие обзоры находят примеры неверной классификации данных, ошибочного дублирования данных, использование полностью неверных наборов данных в анализ и других ошибок, - все спрятаны в статьях, которые не содержат описания проведённого анализа, достаточно подробного для того, чтобы эти ошибки можно было легко заметить. \cite{baggerly_deriving_2009,gotzsche_methodology_1989} 

Солнечный свет - лучшее средство дезинфекции, и многие ученые призывали к тому, что экспериментальные данные должны быть доступны в интернете. В некоторых областях, это стало распространенной практикой: существуют базы данных генных последовательностей, белковых структур, астрономических наблюдений и коллекции данных земных наблюдений, содержащие вклад тысяч различных ученых. Многие другие сферы науки, однако, не могут поделиться своими данными ввиду непрактичности (данные физики элементарных частиц могут содержать терабайты информации), невозможности разглашения (медицинские исследования), отсутствия финансирования или технической поддержки или просто исходя из желания сохранить контроль над данными и всеми открытиями, появляющиеся в результате их анализа. И даже если бы все данные были доступны, стал бы кто-нибудь их анализировать с целью поиска ошибок?   

Схожим образом, ученые в некоторых областях стали делать свой статистический анализ общедоступным путём использования умных технологических инструментов. Например, инструмент под названием Sweave позволяет легко встраивать статистический анализ, сделанный с исопльзованием популярного языка программирования R, в научные статьи, написанные с помощью \LaTeX, считающийся стандартом для написания научных и математических публикаций. Результат выглядит точно также, как и любая научная статья, но другой ученый, прочитавший публикацию и заинтересовавшийся используемыми методами, может скачать исходный код, в котором показано, каким образом были проведены все расчеты. Но будут ли ученые пользоваться такой возможностью? Никто же не достигает научной славы, проверяя код на наличие опечаток.

Другим решением может выступать повторение исследования. Если ученые тщательно воссоздадут ход эксперимента других ученых и подтвердят их результаты, будет намного проще исключить возможность опечатки, которая может привести к ошибочным результатам. Повторение также устраняет случайные ложно положительные результаты. Многие ученые утверждают, что экспериментальное повторение - это основа науки, поскольку ни одна новая идея не принимается до тех пор, пока она не была независимо проверена и перепроверена учеными по всему миру и оказалась способной выдерживать критику.  

Это не совсем верно: ученые часто принимают результаты предыдущих исследований на веру, хотя иногда они принимают решение методично перепроверить предыдущие работы. Например, один новый проект ставит своей целью воспроизвести результаты исследований из крупных психологических журналов, чтобы определить, какое количество статей выдерживают проверку временем и по каким характеристикам статей можно предсказать, насколько данная статья способна выдержать последующие перепроверки.\footnote{The Reproducibility Project, at \href{http://openscienceframework.org/reproducibility/}{}} 

In another example, cancer researchers at Amgen retested 53 landmark preclinical studies in cancer research. (By “preclinical” I mean the studies did not involve human patients, as they were testing new and unproven ideas.) Despite working in collaboration with the authors of the original papers, the Amgen researchers could only reproduce six of the studies.5 Bayer researchers have reported similar difficulties when testing potential new drugs found in published papers.49

This is worrisome. Does the trend hold true for less speculative kinds of medical research? Apparently so: of the top-cited research articles in medicine, a quarter have gone untested after their publication, and a third have been found to be exaggerated or wrong by later research.32 That’s not as extreme as the Amgen result, but it makes you wonder what important errors still lurk unnoticed in important research. Replication is not as prevalent as we would like it to be, and the results are not always favorable.
[1]	The Reproducibility Project, at http://openscienceframework.org/reproducibility/