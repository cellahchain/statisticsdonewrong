%% Chapter 9 %%%
\chapter{Ошибки делают все}
\label{chp9}

До сих пор, я предполагал, что ученые способны выполнять статистические вычисления с идеальной точностью и ошибаться могут только в выборе подходящих для этих вычислений цифр. Ученые могут неправильно использовать результаты статистического анализа или не в состоянии выполнить соответствующие расчеты, но они ведь могут, как минимум, правильно рассчитать \emph{p-}значение?

Возможно, нет. 

Обзоры статистически значимых результатов, представленных в медицинских и психологических исследованиях, показывают, что многие \emph{p-}значения ошибочны, и некоторые статистически незначимые результаты на самом деле значимы, если правильно их пересчитать. \cite{gotzsche_believability_2006,bakker_misreporting_2011} Другие обзоры находят примеры неверной классификации данных, ошибочного дублирования данных, использование полностью неверных наборов данных в анализ и других ошибок, - все спрятаны в статьях, которые не содержат описания проведённого анализа, достаточно подробного для того, чтобы эти ошибки можно было легко заметить. \cite{baggerly_deriving_2009,gotzsche_methodology_1989} 

Солнечный свет - лучшее средство дезинфекции, и многие ученые призывали к тому, что экспериментальные данные должны быть доступны в интернете. В некоторых областях, это стало распространенной практикой: существуют базы данных генных последовательностей, белковых структур, астрономических наблюдений и коллекции данных земных наблюдений, содержащие вклад тысяч различных ученых. Многие другие сферы науки, однако, не могут поделиться своими данными ввиду непрактичности (данные физики элементарных частиц могут содержать терабайты информации), невозможности разглашения (медицинские исследования), отсутствия финансирования или технической поддержки или просто исходя из желания сохранить контроль над данными и всеми открытиями, появляющиеся в результате их анализа. И даже если бы все данные были доступны, стал бы кто-нибудь их анализировать с целью поиска ошибок?   

Схожим образом, ученые в некоторых областях стали делать свой статистический анализ общедоступным путём использования умных технологических инструментов

Similarly, scientists in some fields have pushed towards making their statistical analyses available through clever technological tools. A tool called Sweave, for instance, makes it easy to embed statistical analyses performed using the popular R programming language inside papers written in LaTeX, the standard for scientific and mathematical publications. The result looks just like any scientific paper, but another scientist reading the paper and curious about its methods can download the source code, which shows exactly how all the numbers were calculated. But would scientists avail themselves of the opportunity? Nobody gets scientific glory by checking code for typos.

Another solution might be replication. If scientists carefully recreate the experiments of other scientists and validate their results, it is much easier to rule out the possibility of a typo causing an errant result. Replication also weeds out fluke false positives. Many scientists claim that experimental replication is the heart of science: no new idea is accepted until it has been independently tested and retested around the world and found to hold water.

That’s not entirely true; scientists often take previous studies for granted, though occasionally scientists decide to systematically re-test earlier works. One new project, for example, aims to reproduce papers in major psychology journals to determine just how many papers hold up over time – and what attributes of a paper predict how likely it is to stand up to retesting.[1] In another example, cancer researchers at Amgen retested 53 landmark preclinical studies in cancer research. (By “preclinical” I mean the studies did not involve human patients, as they were testing new and unproven ideas.) Despite working in collaboration with the authors of the original papers, the Amgen researchers could only reproduce six of the studies.5 Bayer researchers have reported similar difficulties when testing potential new drugs found in published papers.49

This is worrisome. Does the trend hold true for less speculative kinds of medical research? Apparently so: of the top-cited research articles in medicine, a quarter have gone untested after their publication, and a third have been found to be exaggerated or wrong by later research.32 That’s not as extreme as the Amgen result, but it makes you wonder what important errors still lurk unnoticed in important research. Replication is not as prevalent as we would like it to be, and the results are not always favorable.
[1]	The Reproducibility Project, at http://openscienceframework.org/reproducibility/