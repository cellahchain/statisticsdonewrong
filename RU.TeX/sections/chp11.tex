%% Chapter 11 %%%
\chapter{К чему мы пришли?}
\label{chp11}

Я обрисовал мрачную картину. Любой может увидеть мелкие детали в публикациях и составить из них огромный список ошибок. Имеют ли значение эти проблемы?

Да, имеют. Иначе я бы не написал всё это.

Знаменитая статья Джона Иоаннидиса ``Почему большинство опубликованных результатов исследований являются ложью''\cite{ioannidis_why_2005} была обоснована и затрагивала в большей степени математические расчеты, нежели эмпирические проверки исследовательских результатов. Если большинство научных статей имеют низкую статистическую мощность - \hyperref[chp3]{а они имеют}, в то время, как у исследователей остается свобода выбора из множества различных методов анализа для получения благоприятных результатов, \hyperref[chp8]{что они и делают}; когда большинство тестируемых гипотез ложны и большинство истинных гипотез соответствуют очень слабым по силе эффектам, мы математически детерминированы на получение массы ложно положительных результатов. 

Но если вам нужен эмпиризм - можете его получить, как сказали Джон Иоаннидис и Джонатан Шоэнфельд. Они изучали вопрос ``Связано ли всё, что мы едим, с раком'' \cite{schoenfeld_is_2013}\footnote{Это важная часть текущего онкологического проекта \href{http://dailymailoncology.tumblr.com/}{Oncological Ontology} по классификации всего на две категории: то, что лечит рак, и то, что его является его причиной.} Выбрав пятьдесят популярных ингридиентов из кулинарной книги, они занялись поиском исследований, связывающих их с показателями заболевания раком, - и обнаружили 216 исследований, описывающих сорок различных ингредиентов. Естественно, большинство исследований противоречили друг другу. Среди исследований оказалось как много защитников, так и противников большинства ингредиентов: одни утверждали, что ингредиент повышает риск заболевания раком, другие постулировали обратное. Большая часть статистических данных было очень слабым, а мета-анализы, как правило, показывали гораздо меньший размер эффекта, в отличие от результатов оригинальных исследований.


Конечно, тот факт, что данные последующих исследований и мета-анализов противоречат результатам одной статьи, не предотвращает дальнейшее её цитирование в других статьях, считающих её результаты истинными. Даже эффекты, которые вступают в противоречие с последующими многочисленными испытаниями с недвусмысленными результатами, всё равно часто цитируются пять или десять лет спустя учёными, которые, по-видимому, не замечают, что эти результаты являются уже ложными. \cite{tatsioni_persistence_2007} Конечно, новые находки получают широкую огласку в прессе, в то время как противоречия или исправления едва ли вообще упоминаются. \cite{gonon_why_2012} Сложно винить учёных за то, что они не в курсе.

Не стоит забывать и о просто ошибочных результатах. Плохие стандарты отчетности в медицинских журналах означают, что исследователи, тестирующие новое лекарство от шизофрении, могут пренебречь включением в статью шкалы, по которой они оценивали проявление симптомов, - неиссякаемый источник ошибки, поскольку результаты с неопубликованными шкалами, как правило, выглядят лучше тех, для получения которых использовались проверенные тесты. \cite{marshall_unpublished_2000} Другие медицинские исследования просто \hyperref[chp10:leaveoutdetails]{не упоминают определённые результаты}, если они не интересны или не благоприятны исследованию, продуцируя, таким образом, ошибки в последующих мета-анализах. По оценкам, примерно треть мета-анализов страдают от этой проблемы. \cite{kirkham_impact_2010}


Another review compared meta-analyses to subsequent large randomized controlled trials, considered the gold standard in medicine. In over a third of cases, the randomized trial’s outcome did not correspond well to the meta-analysis.39 Other comparisons of meta-analyses to subsequent research found that most results were inflated, with perhaps a fifth representing false positives.45

Let’s not forget the multitude of physical science papers which misuse confidence intervals.37 Or the peer-reviewed psychology paper allegedly providing evidence for psychic powers, on the basis of uncontrolled multiple comparisons in exploratory studies.58 Unsurprisingly, results failed to be replicated – by scientists who appear not to have calculated the statistical power of their tests.20

We have a problem. Let’s work on fixing it.
[1]	An important part of the ongoing Oncological Ontology project to categorize everything into two categories: that which cures cancer and that which causes it.